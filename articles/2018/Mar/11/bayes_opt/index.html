<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Data Rounder, ">
    <link href="/favicon.png" rel="icon">

        <link rel="alternate"  href="http://jjakimoto.github.io/feeds/all.atom.xml" type="application/atom+xml" title="Data Rounder Full Atom Feed"/>

        <title>Data Rounder - Bayesian Optimization of Hyperparameters with Python</title>

    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/grids-responsive-old-ie-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="/theme/css/grids-responsive-min.css">
    <!--<![endif]-->
    <link rel="stylesheet" href="/theme/css/styles.css">
    <link rel="stylesheet" href="/theme/css/pygments.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Lato:300,400,700' rel='stylesheet' type='text/css'> -->
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,500" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Ubuntu+Mono' rel='stylesheet' type='text/css'>


    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>


    <header id="header" class="pure-g">
        <div class="pure-u-1 pure-u-md-3-4">
             <div id="menu">
                 <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="/">Home</a></li>
        <li><a href="/pages/books/">Books</a></li>
        <li><a href="/pages/about/">About</a></li>
        <li><a href="/pages/links/">Links</a></li>
</ul>                </div>
            </div>
        </div>

        <div class="pure-u-1 pure-u-md-1-4">
            <div id="social">
                <div class="pure-menu pure-menu-open pure-menu-horizontal">
<ul>
        <li><a href="https://twitter.com/"><i class="fa fa-twitter"></i></a></li>
        <li><a href="https://github.com/"><i class="fa fa-github"></i></a></li>
        <li><a href="https://www.linkedin.com/in/"><i class="fa fa-linkedin"></i></a></li>
        <li><a href="http://jjakimoto.github.io/feeds/all.atom.xml"><i class="fa fa-rss"></i></a></li>
</ul>                </div>
            </div>
        </div>
    </header>



    <div id="layout" class="pure-g">
        <section id="content" class="pure-u-1 pure-u-md-3-4">
            <div class="l-box">

    <header id="post-header">
        <h1>Bayesian Optimization of Hyperparameters with Python</h1>
            <div class="post-meta">
                March 11, 2018
            </div>
            <div class="post-tags">
                <ul>
                    <li>tags:</li>
                    <li><a href="/tag/AutoML/">AutoML</a></li>
                </ul>
            </div>
    </header>

    <section id="post">
        <p>Choosing a good set of hyperparameters is one of most important steps but also pretty much annoying and time consuming. The small number of hyperparameters may allow you to find an optimal hyperparameters after a few trials. This is, however, not the case for complex models like neural network.</p>
<p>Indeed, when I just started my career as a data scientist, I was always frustrated to tune hyperparameters of Neural Network not to either underfit or overfit. Everytime I spent a lot of times and winded up not finding good set of hyperparameters, I was like</p>
<p><img alt="frustration" src="https://media.giphy.com/media/ilkfz8Mn5Yz7O/giphy.gif" /></p>
<p>Actually there were a lot of ways to tune parameters efficiently and algorithmically, which I was ignorant of back in those days. Especially how to tune Neural Network has been progress rapidly in a recent few years by utilizing various algorithms: <a href="https://arxiv.org/pdf/1706.00764.pdf">spectral analysis [1]</a>, <a href="https://arxiv.org/pdf/1603.06560.pdf">bandit algorithms [2]</a>, <a href="https://arxiv.org/pdf/1711.09846.pdf">evolutionary strategy [3]</a>, <a href="https://arxiv.org/pdf/1611.01578.pdf">reinforcement learning [4]</a>, etc. How to build predictive general models algorithmically is also one of the hot research topics. Many frameworks and algorithms have been suggested <a href="https://cyphe.rs/static/atm.pdf">[5]</a>, <a href="https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf">[6]</a>.</p>
<p>Automatically building and tuning models is one of the hot topics in research, some of which are successful in outperforming state-of-art models. Thus, building solid tuning algorithms is way cheaper and more efficient than hiring data scientists for tuning models.</p>
<p><img alt="scientist" src="https://media.giphy.com/media/xUA7b6oaRIgzmAKpUY/giphy.gif" /></p>
<p>In this blog post, we will go through the most basic three algorithms: grid, random, and Bayesian search. And, we will learn how to implement it in python.</p>
<h1>Background</h1>
<p>When optimizing hyperparameters, information available is mostly only score value of defined metrics(e.g., accuracy for classification) with each set of hyperparameters. We query a set of hyperparameters and get a score value as a response. Thus, optimization algorithms have to make efficient queries and find an optimal set without knowing how objective function looks like. This kind of optimization problem is called balck-box optimization. Here is the definition of black-box optimization:</p>
<blockquote>
<p>"Black Box" optimization refers to a problem setup in which an optimization algorithm is supposed to optimize (e.g., minimize) an objective function through a so-called black-box interface: the algorithm may query the value f(x) for a point x, but it does not obtain gradient information, and in particular it cannot make any assumptions on the analytic form of f (e.g., being linear or quadratic). We think of such an objective function as being wrapped in a black-box. The goal of optimization is to find an as good as possible value f(x) within a predefined time, often defined by the number of available queries to the black box. Problems of this type regularly appear in practice, e.g., when optimizing parameters of a model that is either in fact hidden in a black box (e.g., a third party software library) or just too complex to be modeled explicitly.</p>
<p>by <a href="https://bbcomp.ini.rub.de/">Balck-Box Optimization Competition homepage</a>.</p>
</blockquote>
<p>* There are some hyperparameter optimization methods to make use of gradient information, e.g., <a href="http://proceedings.mlr.press/v37/maclaurin15.pdf">[7]</a>.</p>
<p>Grid, random, and Bayesian search, are three of basic algorithms of black-box optimization. They have the following characteristics (We assume the problem is minimization here):</p>
<h2>Grid Search</h2>
<p>Grid search is the simplest method. First, we place finite number of points on each hyperparameter axis and then make grid points by combining them. Here is the example:</p>
<div class="highlight"><pre><span></span><span class="n">A</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">)</span>
<span class="n">B</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span> <span class="p">[(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div>


<p>When you have only a few hyperparameters, this method may works. Once dimension increases, the number of trials blows up exponentially.</p>
<h2>Random Search</h2>
<p>Random search is known effective over high dimensional search space. Especially when we have small subsets of effective hyperparameters out of high dimensional space, we search these effective parameters effectively <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">[8]</a>.</p>
<h2>Bayesian Search</h2>
<p>While random search samples points independently, Bayesian search samples promising points more effectively by utilizing historical results. We first use GP (Gaussian process) to estimate objective function based on historical results <a href="https://arxiv.org/pdf/1206.2944.pdf">[9]</a>.</p>
<p>GP also outputs variance along with mean. If this variance is large, small mean does not necessary imply promising because high values also likely happen as well. Points minimizing mean of estimation function are not necessary optimal. Thus, we need to define metric to consider trade off between mean and variance.</p>
<p>We introduce functions called acquisition function to deal with this issue. One of the most commonly used function is <em>Expected Improvement</em>. Here is the definition:</p>
<p>$$a_{EI}(x; {x_n,  y_n}, \theta) = E[max(f(x_{best}) - f(x), 0) | {x_n,  y_n}, \theta]$$</p>
<p>where $f(\cdot)$ is score function; ${x_n,  y_n}$ historical input and its response from score function; $\theta$ is parameters of Gaussian process; $E[\cdot]$ is taking expectation with respect to a Gaussian probability.</p>
<p>The right hand can be calculated analytically to the following form:</p>
<p>$$a_{EI}(x; {x_n,  y_n}, \theta) = \sigma(x ;  {x_n,  y_n}, \theta) [\gamma(x) \Phi(\gamma(x)) + N (\gamma(x); 0, 1)]$$</p>
<p>where</p>
<p>$$\gamma(x) = \frac{f(x_{best}) − \mu(x ; {x_n,  y_n}, \theta)}{\sigma(x ;  {x_n,  y_n}, \theta)}$$</p>
<p>$N(\cdot; 0, 1)$ and $\Phi(\cdot)$ are p.d.f. and c.d.f of Gaussian distribution, respectively.</p>
<p>Here is python code:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">expected_improvement</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">evaluated_loss</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; expected_improvement</span>
<span class="sd">    Expected improvement acquisition function.</span>

<span class="sd">    Note</span>
<span class="sd">    ----</span>
<span class="sd">    This implementation aims for minimization</span>

<span class="sd">    Parameters:</span>
<span class="sd">    ----------</span>
<span class="sd">    x: array-like, shape = (n_hyperparams,)</span>
<span class="sd">    model: GPRegressor object of GPy.</span>
<span class="sd">        Gaussian process trained on previously evaluated hyperparameters.</span>
<span class="sd">    evaluated_loss: array-like(float), shape = (# historical results,).</span>
<span class="sd">         the values of the loss function for the previously evaluated</span>
<span class="sd">         hyperparameters.</span>
<span class="sd">    jitter: float</span>
<span class="sd">        positive value to make the acquisition more explorative.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Consider 1d case</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Avoid too small sigma</span>
    <span class="k">if</span> <span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_optimum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">evaluated_loss</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_optimum</span> <span class="o">-</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">jitter</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="n">ei_val</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span> <span class="o">+</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">ei_val</span>
</pre></div>


<p>Summing up the above discussion, Bayesian optimization is executed in the following steps:
1. Sample a few points and score them.
2. Initialize GP with sampled points
3. Sample points that minimize acquisition function
4. Score sampled points and store the results in GP
5. Iterate 3. and 4.</p>
<p>To implement them in python, I have implemented two class objects: <a href="https://github.com/jjakimoto/BBOptimizer/tree/develop/bboptimizer/samplers">Sampler</a> and <a href="https://github.com/jjakimoto/BBOptimizer/blob/develop/bboptimizer/optimizer.py">Optimizer</a>.</p>
<p>Sampler class basically consists of two methods:
- update: Update GP based on historical results
- sample: Sample optimal points with respect to an acquisition function</p>
<p>Optimizer class utilizes a sampler to find optimal points.</p>
<p>Here are python codes for the step 3. and 4.:</p>
<p>Step 3.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_bayes_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_restarts</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">init_xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_sample</span><span class="p">(</span><span class="n">num_restarts</span><span class="p">)</span>
    <span class="c1"># Define search space</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">design_space</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">()</span>
    <span class="c1"># Historical results</span>
    <span class="n">evaluated_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Find a point to minimize acquisition function</span>
    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">init_xs</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">acquisition_func</span><span class="p">,</span>
                       <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span>
                       <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
                       <span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span>
                       <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">evaluated_loss</span><span class="p">))</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    <span class="n">best_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xs</span><span class="p">)[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">best_x</span>

<span class="k">def</span> <span class="nf">_random_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="n">Xs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">random_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_conf</span><span class="p">)</span>
        <span class="n">Xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">Xs</span><span class="p">)</span>
</pre></div>


<p>Step 4.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="c1"># Update data in GP</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_XY</span><span class="p">(</span><span class="n">X_vec</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Update hyperparameters of GP</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>


<p><code>self.model.optimize()</code> optimize GP model defined at <a href="https://github.com/SheffieldML/GPy">GPy</a>.
Then, we use these update and sample methods of the sampler object to optimize parameters</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
    <span class="n">Xs</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">Xs</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
    <span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">data</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    <span class="c1"># Default is minimization</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maximize</span><span class="p">:</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="o">-</span><span class="n">ys</span>
    <span class="c1"># Update with  fixed parameters</span>
    <span class="n">best_X</span> <span class="o">=</span> <span class="n">Xs</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">best_y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">best_X</span><span class="p">,</span> <span class="n">best_y</span>
</pre></div>


<p>To make it easy to understand the above codes, I change some parts from actual implementation. If you want to see the full implementation, check out <a href="https://github.com/jjakimoto/BBOptimizer">this repository</a>.</p>
<h1>Experiments</h1>
<p>Let's compare performance of these algorithms!!</p>
<h2>Toy Model</h2>
<p>As a simple example, we shall test the following function:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">map_func</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">linear</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">sin</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">score_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;x2&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="n">map_func</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;x4&quot;</span><span class="p">]](</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;x1&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="n">map_func</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;x4&quot;</span><span class="p">]](</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;x3&quot;</span><span class="p">])</span>
    <span class="n">score_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">score</span>
    <span class="k">return</span> <span class="n">score_val</span>


<span class="n">params_conf</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;num_grid&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;num_grid&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;x3&quot;</span><span class="p">,</span> <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;num_grid&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;x4&quot;</span><span class="p">,</span> <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;sin&quot;</span><span class="p">,</span> <span class="s2">&quot;square&quot;</span><span class="p">),</span>
     <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">},</span>
<span class="p">]</span>
</pre></div>


<p>The <code>x3</code> determines which function is used with for the two  variables: <code>x1</code> and <code>x3</code>. Comparatively speaking, <code>x2</code> does not affect the performance because of sine function. The precise way of defining search space is explained in <a href="https://github.com/jjakimoto/BBOptimizer">my repository</a>.</p>
<p>From the definition above, we know the optimal result in advance:</p>
<div class="highlight"><pre><span></span>params = {&#39;x1&#39;: 5, &#39;x2&#39;: 3.141592..., &#39;x3&#39;, 5.0, &#39;x4&#39;: &#39;sqaure&#39;}
score = 51.0
</pre></div>


<p>To execute optimization, we use</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">bboptimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">bayes_opt</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">score_func</span><span class="p">,</span> <span class="n">params_conf</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;bayes&quot;</span><span class="p">,</span> <span class="n">r_min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">bayes_opt</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random_opt</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">score_func</span><span class="p">,</span> <span class="n">params_conf</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">random_opt</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">grid_opt</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">score_func</span><span class="p">,</span> <span class="n">params_conf</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;grid&quot;</span><span class="p">,</span> <span class="n">num_grid</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">grid_opt</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>


<p>Here is the result:</p>
<p><img alt="toy_model_opt" src="http://jjakimoto.github.io/images/bayes_opt/toy_model_opt.jpg" /></p>
<p>In this example, Bayesian search achieves the almost optimal values:</p>
<div class="highlight"><pre><span></span><span class="n">best_parmas</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="s1">&#39;x3&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="s1">&#39;x4&#39;</span><span class="p">:</span> <span class="s1">&#39;square&#39;</span><span class="p">}</span>
<span class="n">best_score</span> <span class="o">=</span> <span class="mf">50.95892427466314</span>
</pre></div>


<p>after 8 Bayesian samples and 10 random initialization while random and grid search achieve <code>24.004995120648054</code> and <code>25.968924274663138</code> even after 50 trials. In this example, grid search works slightly better than random search. This is because optimal values of <code>x2</code> and <code>x3</code> are placed at the end of search space, which allows grid search to try these values deterministically.</p>
<h2>Hyperparameter Optimization</h2>
<p>Next problem is tuning hyperparameters of one of the basic machine learning models, Support Vector Machine. We consider optimizing regularization parameters <code>C</code> and <code>gamma</code> with accuracy score under fixed kernel to RBF at <code>scikit-learn</code> implementation. We use an artificially classification problem made up with  <code>make_classification</code> of <code>scikit-learn</code>. Let's set up the problem!</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>


<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span>
                                   <span class="n">n_features</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
                                   <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                   <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">score_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">splitter</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">splitter</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">train_target</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">test_idx</span><span class="p">])</span>
    <span class="n">true_y</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_y</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">params_conf</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">),</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;continuous&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span> <span class="s1">&#39;log&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">),</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;continuous&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span> <span class="s1">&#39;log&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;kernel&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;fixed&#39;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>


<p>Here is the results:</p>
<p><img alt="hyper_opt" src="http://jjakimoto.github.io/images/bayes_opt/hyper_opt.jpg" /></p>
<ul>
<li>bayes</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">best_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mf">100000.0</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.03836608377440943</span><span class="p">,</span> <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="s1">&#39;rbf&#39;</span><span class="p">}</span>
<span class="n">best_score</span><span class="o">=</span><span class="mf">0.928</span>
</pre></div>


<ul>
<li>random:</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">best_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mf">196.07647697179934</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.07509896588333721</span><span class="p">,</span> <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="s1">&#39;rbf&#39;</span><span class="p">}</span>
<span class="n">best_score</span><span class="o">=</span> <span class="mf">0.91</span>
</pre></div>


<ul>
<li>grid:</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">best_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mf">4.641588833612772</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.03162277660168379</span><span class="p">,</span> <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="s1">&#39;rbf&#39;</span><span class="p">}</span>
<span class="n">best_score</span><span class="o">=</span><span class="mf">0.904</span>
</pre></div>


<p>As you see in the result above, Bayesian optimization outperformed other algorithms.</p>
<h2>Hyperparameters Optimization Neural Network</h2>
<p>As a final example, we are going to optimize hyperparameters of Neural Network.
For the sake of the simplicity, we define hyperparameters with the following parameters:</p>
<p>For training configuration, we define
- learning rate
- the number of training epochs
- optimization algorithm
- batch size</p>
<p>For each input, hidden, output layers we define
- the number of layers
- the number of hidden units
- weight regularizer
- activation function
- dropout rate
- if use batch normalization</p>
<p>Thus, we have 22 hyperparameters, which is almost infeasible to be optimized by grid search. In this example, we test Bayesian and random search to find good set of 22 hyperparameters.</p>
<p>To test optimization algorithms, we use machine learning "hello world" problem, classifying MNIST handwrite digit data. We fetch data from tensorflow interface and use the train and valid data.</p>
<p>Let's set up the problem:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Reshape</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">Adadelta</span><span class="p">,</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">RMSprop</span>
<span class="kn">from</span> <span class="nn">keras.regularizers</span> <span class="kn">import</span> <span class="n">l2</span>

<span class="kn">from</span> <span class="nn">bboptimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="c1"># Fetch MNIST dataset</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;mnist&quot;</span><span class="p">)</span>


<span class="n">train</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">images</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_y</span><span class="p">)</span>

<span class="n">valid</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">validation</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">valid</span><span class="o">.</span><span class="n">images</span>
<span class="n">valid_X</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">valid_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">valid_y</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">valid_y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_optimzier</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;rmsprop&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">RMSprop</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;sgd&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;adadelta&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Adadelta</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">construct_NN</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">784</span><span class="p">,),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>

    <span class="k">def</span> <span class="nf">update_model</span><span class="p">(</span><span class="n">_model</span><span class="p">,</span> <span class="n">_params</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_drop_rate&quot;</span><span class="p">]))</span>
        <span class="n">_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_num_units&quot;</span><span class="p">],</span>
                    <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_w_reg&quot;</span><span class="p">])))</span>
        <span class="k">if</span> <span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_is_batch&quot;</span><span class="p">]:</span>
            <span class="n">_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_activation&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">_params</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_activation&quot;</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">_model</span>

    <span class="c1"># Add input layer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">update_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="c1"># Add hidden layer</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">]):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">update_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="s2">&quot;hidden&quot;</span><span class="p">)</span>
    <span class="c1"># Add output layer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">update_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_optimzier</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">],</span>
                              <span class="n">lr</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">score_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">construct_NN</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;epochs&quot;</span><span class="p">],</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid_X</span><span class="p">,</span> <span class="n">valid_y</span><span class="p">,</span>
                  <span class="n">batch_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">metrics_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;acc&quot;</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">params_conf</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">250</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span> <span class="s2">&quot;sgd&quot;</span><span class="p">,</span> <span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="s2">&quot;adadelta&quot;</span><span class="p">)},</span>

    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;input_drop_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;input_num_units&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;input_w_reg&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;input_is_batch&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;input_activation&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">)},</span>

    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;hidden_drop_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;hidden_num_units&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;integer&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;hidden_w_reg&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;hidden_is_batch&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;hidden_activation&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">)},</span>

    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;output_drop_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;output_num_units&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;fixed&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;output_w_reg&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="s2">&quot;log&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;output_is_batch&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">)},</span>
    <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;output_activation&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;fixed&quot;</span><span class="p">,</span>
     <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="s2">&quot;softmax&quot;</span><span class="p">},</span>

<span class="p">]</span>
</pre></div>


<p>Here is the result:</p>
<p><img alt="hyper_opt" src="http://jjakimoto.github.io/images/bayes_opt/hyper_nn_opt.jpg" /></p>
<div class="highlight"><pre><span></span><span class="n">bayes</span><span class="p">:</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_hidden_layers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0009053002734681439</span><span class="p">,</span>
               <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">250</span><span class="p">,</span>
               <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
               <span class="s1">&#39;input_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
               <span class="s1">&#39;input_num_units&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
               <span class="s1">&#39;input_w_reg&#39;</span><span class="p">:</span> <span class="mf">1.2840834618450513e-06</span><span class="p">,</span>
               <span class="s1">&#39;input_is_batch&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
               <span class="s1">&#39;input_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
               <span class="s1">&#39;hidden_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
               <span class="s1">&#39;hidden_num_units&#39;</span><span class="p">:</span> <span class="mi">41</span><span class="p">,</span>
               <span class="s1">&#39;hidden_w_reg&#39;</span><span class="p">:</span> <span class="mf">6.970606129393136e-07</span><span class="p">,</span>
               <span class="s1">&#39;hidden_is_batch&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
               <span class="s1">&#39;hidden_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span>
               <span class="s1">&#39;output_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
               <span class="s1">&#39;output_w_reg&#39;</span><span class="p">:</span> <span class="mf">1e-10</span><span class="p">,</span>
               <span class="s1">&#39;output_is_batch&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
               <span class="s1">&#39;output_num_units&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
               <span class="s1">&#39;output_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;softmax&#39;</span><span class="p">}</span>

<span class="n">best_score</span> <span class="o">=</span> <span class="mf">0.9864</span>


<span class="n">random</span><span class="p">:</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">74</span><span class="p">,</span>
               <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">204</span><span class="p">,</span>
               <span class="s1">&#39;hidden_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
               <span class="s1">&#39;hidden_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.6728025784523577</span><span class="p">,</span>
               <span class="s1">&#39;hidden_is_batch&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
               <span class="s1">&#39;hidden_num_units&#39;</span><span class="p">:</span> <span class="mi">45</span><span class="p">,</span>
               <span class="s1">&#39;hidden_w_reg&#39;</span><span class="p">:</span> <span class="mf">1.4924891356983298e-08</span><span class="p">,</span>
               <span class="s1">&#39;input_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span>
               <span class="s1">&#39;input_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.12861674273569668</span><span class="p">,</span>
               <span class="s1">&#39;input_is_batch&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
               <span class="s1">&#39;input_num_units&#39;</span><span class="p">:</span> <span class="mi">92</span><span class="p">,</span>
               <span class="s1">&#39;input_w_reg&#39;</span><span class="p">:</span> <span class="mf">0.00018805052553280536</span><span class="p">,</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0006256532585348427</span><span class="p">,</span>
               <span class="s1">&#39;num_hidden_layers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
               <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
               <span class="s1">&#39;output_activation&#39;</span><span class="p">:</span> <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
               <span class="s1">&#39;output_drop_rate&#39;</span><span class="p">:</span> <span class="mf">0.11413180495018566</span><span class="p">,</span>
               <span class="s1">&#39;output_is_batch&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
               <span class="s1">&#39;output_num_units&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
               <span class="s1">&#39;output_w_reg&#39;</span><span class="p">:</span> <span class="mf">2.544391637336686e-06</span><span class="p">}</span>

<span class="n">best_score</span> <span class="o">=</span> <span class="mf">0.9822000049591064</span>
</pre></div>


<p>In this example, Bayesian search finds that maximum value of the number of epochs is more likely to bring better score and keep using this value from the middle of searching. Then, Bayesian search finds better values more efficiently.</p>
<h1>Wrap Up</h1>
<p>As we go through in this article, Bayesian optimization is easy to implement and efficient to optimize hyperparameters of Machine Learning algorithms. If you have computer resources, I highly recommend you to parallelize processes to speed up <a href="https://arxiv.org/pdf/1602.05149.pdf">[10]</a>. As you have time, you can also try to use Bayesian methods to utilize gradient information <a href="https://arxiv.org/pdf/1703.04389.pdf">[11]</a>.</p>
<h1>References</h1>
<ul>
<li>[0] <a href="https://github.com/jjakimoto/BBOptimizer">BBOptimizer</a></li>
<li>[1] <a href="https://arxiv.org/pdf/1706.00764.pdf">Hyperparameter Optimization: A Spectral Approach</a></li>
<li>[2] <a href="https://arxiv.org/pdf/1603.06560.pdf">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</a></li>
<li>[3] <a href="https://arxiv.org/pdf/1711.09846.pdf">Population Based Training of Neural Networks</a></li>
<li>[4] <a href="https://arxiv.org/pdf/1611.01578.pdf">NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING</a></li>
<li>[5] <a href="https://cyphe.rs/static/atm.pdf">ATM: A distributed, collaborative, scalable system for automated machine learning</a></li>
<li>[6] <a href="https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf">Efficient and Robust Automated Machine Learning</a></li>
<li>[7] <a href="http://proceedings.mlr.press/v37/maclaurin15.pdf">Gradient-based Hyperparameter Optimization through Reversible Learning</a></li>
<li>[8] <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a></li>
<li>[9] <a href="https://arxiv.org/pdf/1206.2944.pdf">PRACTICAL BAYESIAN OPTIMIZATION OF MACHINE LEARNING ALGORITHMS</a></li>
<li>[10] <a href="https://arxiv.org/pdf/1602.05149.pdf">Parallel Bayesian Global Optimization of Expensive Functions</a></li>
<li>[11] <a href="https://arxiv.org/pdf/1703.04389.pdf">Bayesian Optimization with Gradients</a></li>
</ul>
    </section>


<nav class="pagination-wrapper">
    <div class="pagination">
        <div class="pagination-left">
        </div>
        <div class="pagination-right">
                &nbsp;
        </div>
    </div>
</nav>
    <aside class="comments">

<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function () {
        this.page.url = "http://jjakimoto.github.io/articles/2018/Mar/11/bayes_opt/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "bayes_opt"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = '//datarounder.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

    </aside>

            </div>
        </section>

        <nav id="sidebar" class="pure-u-1 pure-u-md-1-4">
            <div class="l-box">
<section>
    <div class="portrait">
            <img src="/theme/images/me.jpg" />
    </div>
    <div class="name-cv">
        <div class="name">Tomoaki Fujii</div>
        <!--div class="cv">
            <a href="/pdfs/cv.pdf"><i class="fa fa-file-pdf-o"></i> CV</a>
        </div-->
    </div>
    <div class="clear"></div>
    <div class="sub-name"></div>
    <div class="contact">f.j.akimoto@gmail.com</div>
</section>

<section class="tags">
    <span class="tag"><a href="/tag/Machine Learning/">#Machine Learning</a></span>
    <span class="tag"><a href="/tag/Deep Learning/">#Deep Learning</a></span>
    <span class="tag"><a href="/tag/Finance/">#Finance</a></span>
    <span class="tag"><a href="/tag/Python/">#Python</a></span>
</section>


<!-- <section class="search">
    <script>
  (function() {
    var cx = '006027474294305969173:cpp1rvft78s';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section> -->            </div>
        </nav>

        <footer id="footer" class="pure-u-1 pure-u-md-3-4">
            <div class="l-box">
                <div>
                    <p>&copy; <a href="http://jjakimoto.github.io">Tomoaki Fujii</a> &ndash;
                        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
                        for <a href="http://blog.getpelican.com/">Pelican</a>
                    </p>
                </div>
            </div>
        </footer>

    </div>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11652802;
var sc_invisible=0;
var sc_security="b99d97e0";
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11652802/0/b99d97e0/0/" alt="Web
Analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>

