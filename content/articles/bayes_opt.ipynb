{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a good set of hyperparameters is one of most important steps but also pretty much annoying and time consuming. The small number of hyperparameters may allow you to find an optimal hyperparameters after a few trials. This is, however, not the case for complex models like neural network. \n",
    "\n",
    "Indeed, when I just started my career as a data scientist, I was always frustrated to tune hyperparameters of Neural Network not to either underfit or overfit. Everytime I spent a lot of times and winded up not finding good set of hyperparameters, I was like\n",
    "\n",
    "![frustration](https://media.giphy.com/media/ilkfz8Mn5Yz7O/giphy.gif)\n",
    "\n",
    "Actually there were a lot of ways to tune parameters efficiently and algorithmically, which I was ignorant of back in those days. Especially how to tune Neural Network has been progress rapidly in a recent few years by utilizing various algorithms: [spectral analysis [1]](https://arxiv.org/pdf/1706.00764.pdf), [bandit algorithms [2]](https://arxiv.org/pdf/1603.06560.pdf), [evolutionary strategy [3]](https://arxiv.org/pdf/1711.09846.pdf), [reinforcement learning [4]](https://arxiv.org/pdf/1611.01578.pdf), etc. How to build predictive general models algorithmically is also one of the hot research topics. Many frameworks and algorithms have been suggested [[5]](https://cyphe.rs/static/atm.pdf), [[6]](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf).\n",
    "\n",
    "Automatically building and tuning models is one of the hot topics in research, some of which are successful in outperforming state-of-art models. Thus, building solid tuning algorithms are way cheaper and more efficient than hiring data scientists for tuning models.\n",
    "\n",
    "![scientist](https://media.giphy.com/media/xUA7b6oaRIgzmAKpUY/giphy.gif)\n",
    "\n",
    "In this blog post, we will go through the most basic three algorithms: grid, random, and Bayesian search. And, we will learn how to implement it in python.\n",
    "\n",
    "# Background\n",
    "When optimizing hyperparameters, information available is mostly only score value of defined metrics(e.g., accuracy for classification) with each set of hyperparameters. We query a set of hyperparameters and get a score value as a response. Thus, optimization algorithms have to make efficient queries and find an optimal set without knowing how objective function looks like. This kind of optimization problem is called balck-box optimization. Here is the definition of black-box optimization:\n",
    "\n",
    "> \"Black Box\" optimization refers to a problem setup in which an optimization algorithm is supposed to optimize (e.g., minimize) an objective function through a so-called black-box interface: the algorithm may query the value f(x) for a point x, but it does not obtain gradient information, and in particular it cannot make any assumptions on the analytic form of f (e.g., being linear or quadratic). We think of such an objective function as being wrapped in a black-box. The goal of optimization is to find an as good as possible value f(x) within a predefined time, often defined by the number of available queries to the black box. Problems of this type regularly appear in practice, e.g., when optimizing parameters of a model that is either in fact hidden in a black box (e.g., a third party software library) or just too complex to be modeled explicitly.\n",
    "\n",
    "> by [Balck-Box Optimization Competition homepage](https://bbcomp.ini.rub.de/).\n",
    "\n",
    "\\* There are some hyperparameter optimization methods to make use of gradient information, e.g., [[7]](http://proceedings.mlr.press/v37/maclaurin15.pdf).\n",
    "\n",
    "Grid, random, and Bayesian search, are three of basic algorithms of black-box optimization. They have the following characteristics (We assume the problem is minimization here):\n",
    " \n",
    "\n",
    "### Grid Search\n",
    "Grid search is the simplest method. First, we place finite number of points on each hyperparameter axis and then make grid points by combining them. Here is the example:\n",
    "```python\n",
    "A: (1e-8, 1e-6, 1e-4)\n",
    "B: (0, 1)\n",
    "(A, B): [(1e-8, 0), (1e-8, 1), (1e-6, 0) (1e-6, 1) (1e-4, 0) (1e-4, 1)]\n",
    "```\n",
    "When you have only a few hyperparameters, this method may works. Once dimension increases, the number of trials blows up exponentially.\n",
    "\n",
    "\n",
    "### Random Search\n",
    "Random search is known effective over high dimensional search space. Especially when we have small subsets of effective hyperparameters out of high dimensional space, we search these effective parameters effectively [[8]](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n",
    "\n",
    "\n",
    "### Bayesian search\n",
    "While random search samples points independently, Bayesian search samples promising points more effectively by utilizing historical results. We first use GP (Gaussian process) to estimate objective function based on historical results [9](https://arxiv.org/pdf/1206.2944.pdf).\n",
    "\n",
    "GP also outputs variance along with mean. If this variance is large, small mean does not necessary imply promising because high values also likely happen as well. Points minimizing mean of estimation function are not necessary optimal. Thus, we need to define metric to consider trade off between mean and variance. \n",
    "\n",
    "We introduce functions called acquisition function to deal with this issue. One of the most commonly used function is _Expected Improvement_. Here is the definition:\n",
    "$$  a_{EI}(x; \\{x_n,  y_n\\}, \\theta) = \\left.E[max(f(x_{best}) - f(x), 0) \\right| \\{x_n,  y_n\\}, \\theta]$$\n",
    "where $f(\\cdot)$ is score function; $\\{x_n,  y_n\\}$ historical input and its response from score function; $\\theta$ is parameters of Gaussian process; $E[\\cdot]$ is taking expectation with respect to a Gaussian probability.\n",
    "\n",
    "\n",
    "The right hand can be calculated analytically to the following form:\n",
    "$$a_{EI}(x; \\{x_n,  y_n\\}, \\theta) = \\sigma(x ;  \\{x_n,  y_n\\}, \\theta) [\\gamma(x) \\Phi(\\gamma(x)) + N (\\gamma(x); 0, 1)]$$\n",
    "where\n",
    "$$\\gamma(x) = \\frac{f(x_{best}) âˆ’ \\mu(x ; \\{x_n,  y_n\\}, \\theta)}{\\sigma(x ;  \\{x_n,  y_n\\}, \\theta)}$$\n",
    "$N(\\cdot; 0, 1)$ and $\\Phi(\\cdot)$ are p.d.f. and c.d.f of Gaussian distribution, respectively.\n",
    "\n",
    "Here is python code:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def expected_improvement(x, model, evaluated_loss, jitter=0.01):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This implementation aims for minimization\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: array-like, shape = (n_hyperparams,)\n",
    "    model: GPRegressor object of GPy.\n",
    "        Gaussian process trained on previously evaluated hyperparameters.\n",
    "    evaluated_loss: array-like(float), shape = (# historical results,).\n",
    "         the values of the loss function for the previously evaluated\n",
    "         hyperparameters.\n",
    "    jitter: float\n",
    "        positive value to make the acquisition more explorative.\n",
    "    \"\"\"\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, var = model.predict(x)\n",
    "    # Consider 1d case\n",
    "    sigma = np.sqrt(var)[0, 0]\n",
    "    mu = mu[0, 0]\n",
    "    # Avoid too small sigma\n",
    "    if sigma == 0.:\n",
    "        return 0.\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "        gamma = (loss_optimum - mu - jitter) / sigma\n",
    "        ei_val = sigma * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
    "        return ei_val\n",
    "```\n",
    "\n",
    "Summing up the above discussion, Bayesian optimization is executed in the following steps:\n",
    "1. Sample a few points and score them.\n",
    "2. Initialize GP with sampled points\n",
    "3. Sample points that minimize acquisition function\n",
    "4. Score sampled points and store the results in GP\n",
    "5. Iterate 3. and 4.\n",
    "\n",
    "To implement them in python, I have implemented two class objects: [Sampler](https://github.com/jjakimoto/BBOptimizer/tree/develop/bboptimizer/samplers) and [Optimizer](https://github.com/jjakimoto/BBOptimizer/blob/develop/bboptimizer/optimizer.py).\n",
    "\n",
    "Sampler class basically consists of two methods:\n",
    "- update: Update GP based on historical results\n",
    "- sample: Sample optimal points with respect to an acquisition function\n",
    "\n",
    "Optimizer class utilizes a sampler to find optimal points.\n",
    "\n",
    "\n",
    "Here are python codes for the step 3. and 4.:\n",
    "\n",
    "Step 3.\n",
    "```python\n",
    "def _bayes_sample(self, num_restarts=25):\n",
    "    init_xs = self._random_sample(num_restarts)\n",
    "    # Define search space \n",
    "    bounds = self.design_space.get_bounds()\n",
    "    # Historical results\n",
    "    evaluated_loss = self._y\n",
    "    ys = []\n",
    "    xs = []\n",
    "\n",
    "    # Find a point to minimize acquisition function\n",
    "    for x0 in init_xs:\n",
    "        res = minimize(fun=-self.acquisition_func,\n",
    "                       x0=x0,\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(self.model, evaluated_loss))\n",
    "        ys.append(-res.fun)\n",
    "        xs.append(res.x)\n",
    "    idx = np.argmax(ys)\n",
    "    best_x = np.array(xs)[idx]\n",
    "    return best_x\n",
    "\n",
    "def _random_sample(self, num_samples):\n",
    "    Xs = []\n",
    "    for i in range(num_samples):\n",
    "        x = random_sample(self.params_conf)\n",
    "        Xs.append(x)\n",
    "    return list(Xs)\n",
    "```\n",
    "\n",
    "Step 4.\n",
    "```python\n",
    "def _update(self, eps=1e-6):\n",
    "    X, y = self.data\n",
    "    y = np.array(y)[:, None]\n",
    "    # Update data in GP\n",
    "    self.model.set_XY(X_vec, y)\n",
    "    # Update hyperparameters of GP\n",
    "    self.model.optimize()\n",
    "```\n",
    "\n",
    "`self.model.optimize()` optimize GP model defined at [GPy](https://github.com/SheffieldML/GPy). \n",
    "Then, we use these update and sample methods of the sampler object to optimize parameters\n",
    "\n",
    "```python\n",
    "for i in range(num_iter):\n",
    "    Xs = sampler.sample(*args, **kwargs)\n",
    "    ys = []\n",
    "    for X in Xs:\n",
    "        y = self.score_func(X)\n",
    "        ys.append(y)\n",
    "        sampler.update(Xs, ys)\n",
    "    Xs, ys = sampler.data\n",
    "    best_idx = np.argmin(ys)\n",
    "    # Default is minimization\n",
    "    if self._maximize:\n",
    "        ys = -ys\n",
    "    # Update with  fixed parameters\n",
    "    best_X = Xs[best_idx]\n",
    "    best_y = ys[best_idx]\n",
    "    return best_X, best_y\n",
    "```\n",
    "To make it easy to understand the above codes, I change some parts from actual implementation. If you want to see the full implementation, check out [this repository](https://github.com/jjakimoto/BBOptimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Let's compare performance of these algorithms!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Model \n",
    "\n",
    "As a simple example, we shall test the following function:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "map_func = dict(linear=lambda x: x, square=lambda x: x**2, sin=np.sin)\n",
    "\n",
    "def score_func(x):\n",
    "    score = np.sin(x[\"x2\"]) + map_func[x[\"x4\"]](x[\"x1\"]) + map_func[x[\"x4\"]](x[\"x3\"])\n",
    "    score_val = -score\n",
    "    return score_val\n",
    "\n",
    "\n",
    "params_conf = [\n",
    "    {\"name\": \"x1\", \"domain\": (.1, 5), \"type\": \"continuous\",\n",
    "     \"num_grid\": 5, \"scale\": \"log\"},\n",
    "    {\"name\": \"x2\", \"domain\": (-5, 3), \"type\": \"continuous\",\n",
    "     \"num_grid\": 5},\n",
    "    {\"name\": \"x3\", \"domain\": (-3, 5), \"type\": \"continuous\",\n",
    "     \"num_grid\": 5},\n",
    "    {\"name\": \"x4\", \"domain\": (\"linear\", \"sin\", \"square\"),\n",
    "     \"type\": \"categorical\"},\n",
    "]\n",
    "```\n",
    "\n",
    "The `x3` determines which function is used with for the two  variables: `x1` and `x3`. Comparatively speaking, `x2` does not affect the performance because of sine function. The precise way of defining search space is explained in [my repository](https://github.com/jjakimoto/BBOptimizer).\n",
    "\n",
    "From the definition above, we know the optimal result in advance:\n",
    "```\n",
    "params = {'x1': 5, 'x2': 3.141592..., 'x3', 5.0, 'x4': 'sqaure'} \n",
    "score = 51.0\n",
    "```\n",
    "\n",
    "To execute optimization, we use \n",
    "```python\n",
    "import random\n",
    "import numpy as np\n",
    "from bboptimizer import Optimizer\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "bayes_opt = Optimizer(score_func, params_conf, sampler=\"bayes\", r_min=10, maximize=True)\n",
    "bayes_opt.search(num_iter=50)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "random_opt = Optimizer(score_func, params_conf, sampler=\"random\", maximize=True)\n",
    "random_opt.search(num_iter=50)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "grid_opt = Optimizer(score_func, params_conf, sampler=\"grid\", num_grid=3, maximize=True)\n",
    "grid_opt.search(num_iter=50)\n",
    "```\n",
    "\n",
    "Here is the result:\n",
    "\n",
    "![toy_model_opt]({filename}/images/bayes_opt/toy_model_opt.jpg)\n",
    "\n",
    "In this example, Bayesian search achieves the almost optimal values:\n",
    "```python\n",
    "best_parmas = {'x1': 5, 'x2': -5.0, 'x3': -5.0, 'x4': 'square'}\n",
    "best_score = 50.95892427466314\n",
    "```\n",
    "after 8 Bayesian samples and 10 random initialization while random and grid search achieve `24.004995120648054` and `25.968924274663138` even after 50 trials. In this example, grid search works slightly better than random search. This is because optimal values of `x2` and `x3` are placed at the end of search space, which allows grid search to try these values deterministically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "Next problem is tuning hyperparameters of one of the basic machine learning models, Support Vector Machine. We consider optimizing regularization parameters `C` and `gamma` with accuracy score under fixed kernel to RBF at `scikit-learn` implementation. We use an artificially classification problem made up with  `make_classification` of `scikit-learn`. Let's set up the problem!\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "data, target = make_classification(n_samples=2500,\n",
    "                                   n_features=45,\n",
    "                                   n_informative=5,\n",
    "                                   n_redundant=5)\n",
    "\n",
    "\n",
    "def score_func(params):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    train_idx, test_idx = list(splitter.split(data, target))[0]\n",
    "    train_data = data[train_idx]\n",
    "    train_target = target[train_idx]\n",
    "    clf = SVC(**params)\n",
    "    clf.fit(train_data, train_target)\n",
    "    pred = clf.predict(data[test_idx])\n",
    "    true_y = target[test_idx]\n",
    "    score = accuracy_score(true_y, pred)\n",
    "    return score\n",
    "\n",
    "params_conf = [\n",
    "    {'name': 'C', 'domain': (1e-8, 1e5), 'type': 'continuous', 'scale': 'log'},\n",
    "    {'name': 'gamma', 'domain': (1e-8, 1e5), 'type': 'continuous', 'scale': 'log'},\n",
    "    {'name': 'kernel', 'domain': 'rbf', 'type': 'fixed'}\n",
    "]\n",
    "```\n",
    "\n",
    "Here is the results:\n",
    "\n",
    "![hyper_opt]({filename}/images/bayes_opt/hyper_opt.jpg)\n",
    "\n",
    "- bayes\n",
    "```python\n",
    "best_params={'C': 100000.0, 'gamma': 0.03836608377440943, 'kernel': 'rbf'}\n",
    "best_score=0.928\n",
    "```\n",
    "- random:\n",
    "```python\n",
    "best_params={'C': 196.07647697179934, 'gamma': 0.07509896588333721, 'kernel': 'rbf'}\n",
    "best_score= 0.91\n",
    "```\n",
    "- grid:\n",
    "```python\n",
    "best_params={'C': 4.641588833612772, 'gamma': 0.03162277660168379, 'kernel': 'rbf'}\n",
    "best_score=0.904\n",
    "```\n",
    "\n",
    "As you see in the result above, Bayesian optimization outperformed other algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network  Optimization\n",
    "\n",
    "As a final example, we are going to optimize hyperparameters of Neural Network.\n",
    "For the sake of the simplicity, we define hyperparameters with the following parameters:\n",
    "\n",
    "For training configuration, we define\n",
    "- learning rate\n",
    "- the number of training epochs\n",
    "- optimization algorithm\n",
    "- batch size\n",
    "\n",
    "For each input, hidden, output layers we define\n",
    "- the number of layers\n",
    "- the number of hidden units\n",
    "- weight regularizer\n",
    "- activation function \n",
    "- dropout rate\n",
    "- if use batch normalization\n",
    "\n",
    "Thus, we have 22 hyperparameters, which is almost infeasible to be optimized by grid search. In this example, we test Bayesian and random search to find good set of 22 hyperparameters.\n",
    "\n",
    "To test optimization algorithms, we use machine learning \"hello world\" problem, classifying MNIST handwrite digit data. We fetch data from tensorflow interface and use the train and valid data.\n",
    "\n",
    "Let's set up the problem:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.layers import Activation, Reshape\n",
    "from keras.optimizers import Adam, Adadelta, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from bboptimizer import Optimizer\n",
    "\n",
    "# Fetch MNIST dataset\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "\n",
    "train = mnist.train\n",
    "X = train.images\n",
    "train_X = X\n",
    "train_y = np.expand_dims(train.labels, -1)\n",
    "train_y = OneHotEncoder().fit_transform(train_y)\n",
    "\n",
    "valid = mnist.validation\n",
    "X = valid.images\n",
    "valid_X = X \n",
    "valid_y = np.expand_dims(valid.labels, -1)\n",
    "valid_y = OneHotEncoder().fit_transform(valid_y)\n",
    "\n",
    "\n",
    "def get_optimzier(name, **kwargs):\n",
    "    if name == \"rmsprop\":\n",
    "        return RMSprop(**kwargs)\n",
    "    elif name == \"adam\":\n",
    "        return Adam(**kwargs)\n",
    "    elif name == \"sgd\":\n",
    "        return SGD(**kwargs)\n",
    "    elif name == \"adadelta\":\n",
    "        return Adadelta(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "\n",
    "\n",
    "def construct_NN(params):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((784,), input_shape=(784,)))\n",
    "    \n",
    "    def update_model(_model, _params, name):\n",
    "        _model.add(Dropout(_params[name + \"_drop_rate\"]))\n",
    "        _model.add(Dense(units=_params[name + \"_num_units\"],\n",
    "                    activation=None,\n",
    "                    kernel_regularizer=l2(_params[name + \"_w_reg\"])))\n",
    "        if _params[name + \"_is_batch\"]:\n",
    "            _model.add(BatchNormalization())\n",
    "        if _params[name + \"_activation\"] is not None:\n",
    "            _model.add(Activation(_params[name + \"_activation\"]))\n",
    "        return _model\n",
    "    \n",
    "    # Add input layer    \n",
    "    model = update_model(model, params, \"input\")\n",
    "    # Add hidden layer\n",
    "    for i in range(params[\"num_hidden_layers\"]):\n",
    "        model = update_model(model, params, \"hidden\")\n",
    "    # Add output layer\n",
    "    model = update_model(model, params, \"output\")\n",
    "    optimizer = get_optimzier(params[\"optimizer\"],\n",
    "                              lr=params[\"learning_rate\"])\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "        \n",
    "\n",
    "def score_func(params):\n",
    "    model = construct_NN(params)\n",
    "    model.fit(train_X, train_y,\n",
    "              epochs=params[\"epochs\"],\n",
    "              batch_size=params[\"batch_size\"], verbose=1)\n",
    "    score = model.evaluate(valid_X, valid_y,\n",
    "                  batch_size=params[\"batch_size\"])\n",
    "    idx = model.metrics_names.index(\"acc\")\n",
    "    score = score[idx]\n",
    "    print(params, score)\n",
    "    return score\n",
    "\n",
    "params_conf = [\n",
    "    {\"name\": \"num_hidden_layers\", \"type\": \"integer\",\n",
    "     \"domain\": (0, 5)},\n",
    "    {\"name\": \"batch_size\", \"type\": \"integer\",\n",
    "     \"domain\": (16, 128), \"scale\": \"log\"},\n",
    "    {\"name\": \"learning_rate\", \"type\": \"continuous\",\n",
    "     \"domain\": (1e-5, 1e-1), \"scale\": \"log\"},\n",
    "    {\"name\": \"epochs\", \"type\": \"integer\",\n",
    "     \"domain\": (10, 250), \"scale\": \"log\"},\n",
    "    {\"name\": \"optimizer\", \"type\": \"categorical\",\n",
    "     \"domain\": (\"rmsprop\", \"sgd\", \"adam\", \"adadelta\")},\n",
    "    \n",
    "    {\"name\": \"input_drop_rate\", \"type\": \"continuous\",\n",
    "     \"domain\": (0, 0.5)},\n",
    "    {\"name\": \"input_num_units\", \"type\": \"integer\",\n",
    "     \"domain\": (32, 512), \"scale\": \"log\"},\n",
    "    {\"name\": \"input_w_reg\", \"type\": \"continuous\",\n",
    "     \"domain\": (1e-10, 1e-1), \"scale\": \"log\"},\n",
    "    {\"name\": \"input_is_batch\", \"type\": \"categorical\",\n",
    "     \"domain\": (True, False)},\n",
    "    {\"name\": \"input_activation\", \"type\": \"categorical\",\n",
    "     \"domain\": (\"relu\", \"sigmoid\", \"tanh\")},\n",
    "    \n",
    "    {\"name\": \"hidden_drop_rate\", \"type\": \"continuous\",\n",
    "     \"domain\": (0, 0.75)},\n",
    "    {\"name\": \"hidden_num_units\", \"type\": \"integer\",\n",
    "     \"domain\": (32, 512), \"scale\": \"log\"},\n",
    "    {\"name\": \"hidden_w_reg\", \"type\": \"continuous\",\n",
    "     \"domain\": (1e-10, 1e-1), \"scale\": \"log\"},\n",
    "    {\"name\": \"hidden_is_batch\", \"type\": \"categorical\",\n",
    "     \"domain\": (True, False)},\n",
    "    {\"name\": \"hidden_activation\", \"type\": \"categorical\",\n",
    "     \"domain\": (\"relu\", \"sigmoid\", \"tanh\")},\n",
    "    \n",
    "    {\"name\": \"output_drop_rate\", \"type\": \"continuous\",\n",
    "     \"domain\": (0, 0.5)},\n",
    "    {\"name\": \"output_num_units\", \"type\": \"fixed\",\n",
    "     \"domain\": 10},\n",
    "    {\"name\": \"output_w_reg\", \"type\": \"continuous\",\n",
    "     \"domain\": (1e-10, 1e-1), \"scale\": \"log\"},\n",
    "    {\"name\": \"output_is_batch\", \"type\": \"categorical\",\n",
    "     \"domain\": (True, False)},\n",
    "    {\"name\": \"output_activation\", \"type\": \"fixed\",\n",
    "     \"domain\": \"softmax\"},\n",
    "    \n",
    "]\n",
    "```\n",
    "\n",
    "Here is the result:\n",
    "\n",
    "![hyper_opt]({filename}/images/bayes_opt/hyper_nn_opt.jpg)\n",
    "\n",
    "```python\n",
    "bayes:\n",
    "best_params = {'num_hidden_layers': 0,\n",
    "               'batch_size': 128,\n",
    "               'learning_rate': 0.0009053002734681439,\n",
    "               'epochs': 250,\n",
    "               'optimizer': 'rmsprop',\n",
    "               'input_drop_rate': 0.5,\n",
    "               'input_num_units': 512,\n",
    "               'input_w_reg': 1.2840834618450513e-06,\n",
    "               'input_is_batch': True,\n",
    "               'input_activation': 'sigmoid',\n",
    "               'hidden_drop_rate': 0.0,\n",
    "               'hidden_num_units': 41,\n",
    "               'hidden_w_reg': 6.970606129393136e-07,\n",
    "               'hidden_is_batch': True,\n",
    "               'hidden_activation': 'relu',\n",
    "               'output_drop_rate': 0.0,\n",
    "               'output_w_reg': 1e-10,\n",
    "               'output_is_batch': True,\n",
    "               'output_num_units': 10,\n",
    "               'output_activation': 'softmax'}\n",
    "\n",
    "best_score = 0.9864\n",
    "\n",
    "\n",
    "random:\n",
    "best_params = {'batch_size': 74,\n",
    "               'epochs': 204,\n",
    "               'hidden_activation': 'tanh',\n",
    "               'hidden_drop_rate': 0.6728025784523577,\n",
    "               'hidden_is_batch': False,\n",
    "               'hidden_num_units': 45,\n",
    "               'hidden_w_reg': 1.4924891356983298e-08,\n",
    "               'input_activation': 'relu',\n",
    "               'input_drop_rate': 0.12861674273569668,\n",
    "               'input_is_batch': True,\n",
    "               'input_num_units': 92,\n",
    "               'input_w_reg': 0.00018805052553280536,\n",
    "               'learning_rate': 0.0006256532585348427,\n",
    "               'num_hidden_layers': 0,\n",
    "               'optimizer': 'adam',\n",
    "               'output_activation': 'softmax',\n",
    "               'output_drop_rate': 0.11413180495018566,\n",
    "               'output_is_batch': False,\n",
    "               'output_num_units': 10,\n",
    "               'output_w_reg': 2.544391637336686e-06}\n",
    "               \n",
    "best_score = 0.9822000049591064\n",
    "```\n",
    "\n",
    "In this example, Bayesian search finds that maximum value of the number of epochs is more likely to bring better score and keep using this value from the middle of searching. Then, Bayesian search finds better values more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Up\n",
    "\n",
    "As we go through in this article, Bayesian optimization is easy to implement and efficient to optimize hyperparameters of Machine Learning algorithms. If you have computer resources, I highly recommend you to parallelize processes to speed up [[10]](https://arxiv.org/pdf/1602.05149.pdf). As you have time, you can also try to use Bayesian methods to utilize gradient information [[11]](https://arxiv.org/pdf/1703.04389.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [0] [BBOptimizer](https://github.com/jjakimoto/BBOptimizer)\n",
    "- [1] [Hyperparameter Optimization: A Spectral Approach](https://arxiv.org/pdf/1706.00764.pdf)\n",
    "- [2] [Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization](https://arxiv.org/pdf/1603.06560.pdf)\n",
    "- [3] [Population Based Training of Neural Networks](https://arxiv.org/pdf/1711.09846.pdf)\n",
    "- [4] [NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING](https://arxiv.org/pdf/1611.01578.pdf)\n",
    "- [5] [ATM: A distributed, collaborative, scalable system for automated machine learning](https://cyphe.rs/static/atm.pdf)\n",
    "- [6] [Efficient and Robust Automated Machine Learning](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)\n",
    "- [7] [Gradient-based Hyperparameter Optimization through Reversible Learning](http://proceedings.mlr.press/v37/maclaurin15.pdf)\n",
    "- [8] [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "- [9] [PRACTICAL BAYESIAN OPTIMIZATION OF MACHINE LEARNING ALGORITHMS](https://arxiv.org/pdf/1206.2944.pdf)\n",
    "- [10] [Parallel Bayesian Global Optimization of Expensive Functions](https://arxiv.org/pdf/1602.05149.pdf)\n",
    "- [11] [Bayesian Optimization with Gradients](https://arxiv.org/pdf/1703.04389.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
