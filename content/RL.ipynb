{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we cound not fetch data from the following companies\n",
      "['ABBV', 'ALLE', 'CFG', 'COTY', 'CSRA', 'EVHC', 'FB', 'FTV', 'HPE', 'KHC', 'MNK', 'NAVI', 'NWSA', 'NWS', 'NEE', 'PYPL', 'PSX', 'QRVO', 'SYF', 'UA', 'WRK', 'WLTW', 'ZTS']\n",
      "CPU times: user 10.6 s, sys: 1.2 s, total: 11.8 s\n",
      "Wall time: 9min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "symbols = utils.get_sap_symbols('sap500')\n",
    "start_date=\"2012-01-01\"\n",
    "end_date=\"2017-01-01\"\n",
    "# use Open data\n",
    "input_data = utils.get_data_list_key(symbols, start_date, end_date, key='Volume')\n",
    "target_data = utils.get_data('^OEX', start_date, end_date)['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 482)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vol = np.mean(input_data.values, axis=0)\n",
    "notsym = ['ABBV', 'ALLE', 'CFG', 'COTY', 'CSRA', 'EVHC', 'FB', 'FTV', 'HPE', 'KHC', 'MNK', 'NAVI', 'NWSA', 'NWS', 'NEE', 'PYPL', 'PSX', 'QRVO', 'SYF', 'UA', 'WRK', 'WLTW', 'ZTS']\n",
    "symbols = [s for s in symbols if s not in notsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symvol = zip(symbols, vol)\n",
    "symvol = sorted(symvol, key=lambda x: x[1], reverse=True)\n",
    "chosen_symbol = [x[0] for x in symvol[:10]]\n",
    "chosen_volume = [x[1] for x in symvol[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAHXCAYAAADnb7C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1216ae4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.arange(len(chosen_symbol))  \n",
    "height = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5,5])\n",
    "_ = ax.barh(idx, chosen_volume[::-1], height)\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_xlabel('Volume')\n",
    "ax.set_title('The 50 biggest volume companies')\n",
    "ax.set_yticks(idx + height)\n",
    "_ = ax.set_yticklabels(chosen_symbol[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_symbol = ['BAC', 'AAPL', 'GE', 'MSFT', 'F', 'CSCO', 'INTC', 'HPQ', 'PFE', 'MU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAC', 'AAPL', 'GE', 'MSFT', 'F', 'CSCO', 'INTC', 'HPQ', 'PFE', 'MU']\n"
     ]
    }
   ],
   "source": [
    "print(chosen_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 612 ms, sys: 40 ms, total: 652 ms\n",
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "st = time.time()\n",
    "symbols = utils.get_sap_symbols('sap500')\n",
    "start_date=\"2012-01-01\"\n",
    "end_date=\"2017-01-01\"\n",
    "# use Open data\n",
    "input_data = utils.get_data_list_key(chosen_symbol, start_date, end_date)\n",
    "target_data = utils.get_data('^OEX', start_date, end_date)['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "Experiecne = namedtuple('Experience', 'state0,  action, reward, state1')\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        # self.data = [None for _ in range(maxlen)]\n",
    "        self.data = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx< 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "    \n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item\n",
    "            self.data[:-1] = self.data[1:]\n",
    "        else:\n",
    "            # This should never happen\n",
    "            raise RuntimeError()\n",
    "        self.data.append(v)\n",
    "        \n",
    "class SequentialMemory(object):\n",
    "    def __init__(self, limit=1000):\n",
    "        self.limit = limit\n",
    "        self.priority = []\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "        self.batch_idx = None\n",
    "\n",
    "        \n",
    "    def sample(self, batch_size, window_length, alpha=1.0, beta=1.0, epsilon=0.05):\n",
    "        # udpate priority when sampling\n",
    "        if len(self.priority) > self.limit:\n",
    "            self.priority = self.priority[-self.limit:]\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        index_space = np.arange(window_length, self.nb_entries)\n",
    "        # prioritized sample\n",
    "        p = np.array(self.priority)[window_length:]\n",
    "        p_tilde = p + np.ones(self.nb_entries - window_length) * np.mean(p) * epsilon\n",
    "        p_tilde[-1] = np.mean(p)\n",
    "        p_tilde = p_tilde ** alpha\n",
    "        p_tilde = p_tilde / np.sum(p_tilde)\n",
    "        batch_idx = choice(index_space, p=p_tilde, size=batch_size - 1)\n",
    "        # take the newest data\n",
    "        batch_idx = np.concatenate((batch_idx, [self.nb_entries - 1]))\n",
    "        assert len(batch_idx) == batch_size\n",
    "        # keep batch_idx to update pritority\n",
    "        self.batch_idx = batch_idx\n",
    "        \n",
    "        # weights to modify biased update\n",
    "        weights = 1. / (p_tilde**beta)\n",
    "        weights = weights / np.max(weights)\n",
    "        ret_w = weights[batch_idx - window_length]\n",
    "        \n",
    "        # create experiences\n",
    "        state0 = np.array([[self.observations[i] for i in range(idx - window_length,idx)] for idx in batch_idx])\n",
    "        action = np.array([self.actions[idx - 1] for idx in batch_idx])\n",
    "        reward = np.array([self.rewards[idx - 1] for idx in batch_idx])\n",
    "        state1 = np.array([[self.observations[i] for i in range(idx - window_length + 1,idx + 1)] for idx in batch_idx])\n",
    "        return Experiecne(state0, action, reward, state1), ret_w\n",
    "    \n",
    "    def sample_state(self, batch_size, window_length, alpha=0.5, epsilon=0.05):\n",
    "        # udpate priority when sampling\n",
    "        if len(self.priority) > self.limit:\n",
    "            self.priority = self.priority[-self.limit:]\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        index_space = np.arange(window_length, self.nb_entries)\n",
    "        # prioritized sample\n",
    "        p = np.array(self.priority)[window_length:]\n",
    "        p_tilde = p + np.ones(self.nb_entries - window_length) * np.mean(p) * epsilon\n",
    "        p_tilde[-1] = np.mean(p)\n",
    "        p_tilde = p_tilde ** alpha\n",
    "        p_tilde = p_tilde / np.sum(p_tilde)\n",
    "        batch_idx = choice(index_space, p=p_tilde, size=batch_size - 1)\n",
    "        # take the newest data\n",
    "        batch_idx = np.concatenate((batch_idx, [self.nb_entries]))\n",
    "        assert len(batch_idx) == batch_size\n",
    "        \n",
    "        # create experiences\n",
    "        state = np.array([[self.observations[i] for i in range(idx - window_length,idx)] for idx in batch_idx])\n",
    "        return state\n",
    "    \n",
    "    def sample_state_uniform(self, batch_size, window_length):\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        batch_idx = np.random.random_integers(window_length, self.nb_entries - 1, size=batch_size - 1)\n",
    "        # take the newest data\n",
    "        batch_idx = np.concatenate((batch_idx, [self.nb_entries]))\n",
    "        assert len(batch_idx) == batch_size\n",
    "        \n",
    "        # create experiences\n",
    "        state = np.array([[self.observations[i] for i in range(idx - window_length, idx)] for idx in batch_idx])\n",
    "        return state\n",
    "    \n",
    "    def update_priority(self,error):\n",
    "        for idx, i in enumerate(self.batch_idx):\n",
    "            self.priority[i] = error[idx]\n",
    "    \n",
    "    \n",
    "    def append(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        # initialize new sample with 1\n",
    "        self.priority.append(1.0)\n",
    "    \n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return  len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# local library\n",
    "from memory import SequentialMemory\n",
    "\n",
    "class DDPG(object):\n",
    "    \"\"\"Deep Deterministic Poilicy Gradient\n",
    "    \n",
    "    Basend on DDPG and Multiscale CNN, seek out \n",
    "    optimal strategy for stock trading.\n",
    "    \n",
    "    Available function\n",
    "    - build_model: build network based on tensorflow and keras\n",
    "    - train: given DateFrame stock data, train network\n",
    "    - predict_action: givne DataFrame stock data, return optimal protfolio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        \n",
    "        config should have the following attributes\n",
    "        \n",
    "        Args:\n",
    "            device: the device to use computation, e.g. '/gpu:0'\n",
    "            gamma(float): the decay rate for value at RL\n",
    "            history_length(int): input_length for each scale at CNN\n",
    "            n_feature(int): the number of type of input \n",
    "                (e.g. the number of company to use at stock trading)\n",
    "            trade_stock_idx(int): trading stock index\n",
    "            gam (float): discount factor\n",
    "            n_history(int): the nubmer of history that will be used as input\n",
    "            n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n",
    "            k_w(int): the size of filter at CNN\n",
    "            n_hidden(int): the size of fully connected layer\n",
    "            n_batch(int): the size of mini batch\n",
    "            n_epochs(int): the training epoch for each time\n",
    "            update_rate (0, 1): parameter for soft update\n",
    "            learning_rate(float): learning rate for SGD\n",
    "            memory_length(int): the length of Replay Memory\n",
    "            n_memory(int): the number of different Replay Memories\n",
    "            alpha, beta: [0, 1] parameters for Prioritized Replay Memories\n",
    "            action_scale(float): the scale of initialized ation\n",
    "        \"\"\"\n",
    "        self.device = config.device\n",
    "        self.save_path = config.save_path\n",
    "        self.is_load = config.is_load\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_stock = config.n_stock\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_down = config.n_down\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.update_rate = config.update_rate\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.lr = config.learning_rate\n",
    "        self.memory_length = config.memory_length\n",
    "        self.n_memory = config.n_memory\n",
    "        self.noise_scale = config.noise_scale\n",
    "        self.model_config = config.model_config\n",
    "        # the length of the data as input\n",
    "        self.n_history = max(self.n_smooth + self.history_length, (self.n_down + 1) * self.history_length)\n",
    "        print (\"building model....\")\n",
    "        # have compatibility with new tensorflow\n",
    "        tf.python.control_flow_ops = tf\n",
    "        # avoid creating _LEARNING_PHASE outside the network\n",
    "        K.clear_session()\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        K.set_session(self.sess)\n",
    "        with self.sess.as_default():\n",
    "            with tf.device(self.device):\n",
    "                self.build_model()\n",
    "        print('finished building model!')\n",
    "    \n",
    "    def train(self, input_data):\n",
    "        self.max_action = 100\n",
    "        \"\"\"training DDPG, where action is confined to integer space\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): stock price for self.n_feature companies\n",
    "        \"\"\"\n",
    "        stock_data = input_data.values\n",
    "        date = input_data.index\n",
    "        T = len(stock_data)\n",
    "        \n",
    "        # frequency for output\n",
    "        print_freq = int(T / 10)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "        print_freq = 1\n",
    "            \n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        # prioritizomg parameter\n",
    "        db = (1 - self.beta) / 1000\n",
    "        \n",
    "        # result for return value\n",
    "        values = []\n",
    "        date_label = []\n",
    "        value = 0\n",
    "        values.append(value)\n",
    "        date_label.append(date[0])\n",
    "        # keep half an year data \n",
    "        t0 = self.n_history + self.n_batch\n",
    "        self.initialize_memory(stock_data[:t0])\n",
    "        plot_freq = 10\n",
    "        save_freq = 10\n",
    "        count = 0\n",
    "        for t in range(t0, T - 1):\n",
    "            self.update_memory(stock_data[t], stock_data[t+1])\n",
    "            reward = self.take_action(stock_data[t], stock_data[t+1])\n",
    "            value += reward\n",
    "            date_label.append(date[t+1])\n",
    "            values.append(value)\n",
    "            count += 1\n",
    "            for epoch in range(self.n_epoch):    \n",
    "                # select transition from pool\n",
    "                self.update_weight()\n",
    "                # update prioritizing paramter untill it goes over 1\n",
    "                # self.beta  += db\n",
    "                if self.beta >= 1.0:\n",
    "                    self.beta = 1.0\n",
    "                 \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  date[t + 1])\n",
    "                action = self.predict_action(stock_data[t+1])\n",
    "                print(\"portfolio:\", action)\n",
    "                print(\"reward:\", reward)\n",
    "                print(\"value:\", value)\n",
    "                print (\"elapsed time\", time.time() - st)\n",
    "                print(\"********************************************************************\")\n",
    "                \n",
    "            if count % plot_freq == 0:\n",
    "                result = pd.DataFrame(values, index=pd.DatetimeIndex(date_label))\n",
    "                result.to_csv(\"training_result.csv\")\n",
    "                \n",
    "            if count % save_freq == 0:\n",
    "                save_path = self.saver.save(self.sess, self.save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "\n",
    "        save_path = self.saver.save(self.sess, self.save_path)\n",
    "        print(\"Model saved in file: %s\" % self.save_path)\n",
    "        print (\"finished training\")\n",
    "           \n",
    "        return pd.DataFrame(values, index=pd.DatetimeIndex(date_label))\n",
    "    \n",
    "    def norm_action(self, action):\n",
    "        max_action = np.max(np.abs(action))\n",
    "        if max_action > 1:\n",
    "            return action / max_action\n",
    "        else:\n",
    "            return action\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Preduct Optimal Portfolio\n",
    "        \n",
    "        Args:\n",
    "            state(float): stock data with size: [self.n_stock, ]\n",
    "        Retrun:\n",
    "            np.array with size: [self.n_stock, ]\n",
    "        \"\"\"\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        new_state = pred_state[-1]\n",
    "        new_state = np.concatenate((new_state[1:], [state]), axis=0)\n",
    "        pred_state = np.concatenate((pred_state[:-1], [new_state]), axis=0)\n",
    "        action = self.actor_output.eval(\n",
    "            session=self.sess,\n",
    "            feed_dict={self.state: pred_state, K.learning_phase(): 0})[-1]\n",
    "        # action = self.norm_action(action)\n",
    "        return action\n",
    "    \n",
    "    def update_weight(self):\n",
    "        # pararel memory update\n",
    "        idx = np.random.randint(0, self.n_memory)\n",
    "        experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n",
    "        self.sess.run(self.critic_optim, \n",
    "                      feed_dict={self.state: experiences.state0,\n",
    "                                 self.state_target: experiences.state1,\n",
    "                                 self.reward: experiences.reward,\n",
    "                                 self.action: experiences.action,\n",
    "                                 self.weights: weights,\n",
    "                                 self.learning_rate: self.lr,\n",
    "                                 K.learning_phase(): 1})  \n",
    "        self.sess.run(self.actor_optim,\n",
    "                      feed_dict={self.state: experiences.state0,\n",
    "                                 self.learning_rate: self.lr,\n",
    "                                 K.learning_phase(): 1})  \n",
    "                \n",
    "        error = self.sess.run(self.error,\n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.state_target: experiences.state1,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         self.action: experiences.action,\n",
    "                                         K.learning_phase(): 0})\n",
    "        self.memory[idx].update_priority(error)\n",
    "                    \n",
    "        # softupdate for critic network\n",
    "        old_weights = self.critic_target.get_weights()\n",
    "        new_weights = self.critic.get_weights()\n",
    "        weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w\n",
    "                   for new_w, old_w in zip(new_weights, old_weights)]\n",
    "        self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def initialize_memory(self, stocks):\n",
    "        self.memory = []\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory.append(SequentialMemory(self.memory_length))\n",
    "        for t in range(len(stocks) - 1):\n",
    "            for idx_memory in range(self.n_memory):\n",
    "                action = np.random.normal(0, self.noise_scale, self.n_stock)\n",
    "                action = self.norm_action(action)\n",
    "                reward = np.sum((stocks[t + 1] - stocks[t]) * action)\n",
    "                self.memory[idx_memory].append(stocks[t], action, reward)\n",
    "        \n",
    "    def update_memory(self, state, state_forward):\n",
    "        # update memory without updating weight\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory[i].observations.append(state)\n",
    "            self.memory[i].priority.append(1.0)\n",
    "        # to stabilize batch normalization, use other samples for prediction\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        # off policy action and update portfolio\n",
    "        actor_action = self.actor_output.eval(session=self.sess,\n",
    "                                      feed_dict={self.state: pred_state,\n",
    "                                                          K.learning_phase(): 0})[-1]\n",
    "        # action_off = np.round(actor_value_off + np.random.normal(0, noise_scale, self.n_stock))\n",
    "        for i in range(self.n_memory):\n",
    "            action_off = actor_action + np.random.normal(0, self.noise_scale, self.n_stock)\n",
    "            action_off = self.norm_action(action_off)\n",
    "            # action_off = actor_value_off\n",
    "            reward_off = reward = np.sum((state_forward - state) * action_off)\n",
    "            self.memory[i].rewards.append(reward_off)\n",
    "            self.memory[i].actions.append(action_off)\n",
    "       \n",
    "    def take_action(self, state, state_forward):\n",
    "        # to stabilize batch normalization, use other samples for prediction\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        # off policy action and update portfolio\n",
    "        action = self.actor_output.eval(session=self.sess,\n",
    "                                      feed_dict={self.state: pred_state,\n",
    "                                                          K.learning_phase(): 0})[-1]\n",
    "        reward = np.sum((state_forward - state) * action)\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled, action]\n",
    "        actor network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # actor network input should be [raw_data, smoothed, downsampled]\n",
    "        self.actor = self.build_actor()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\n",
    "        raw, smoothed, down = self.transform_input(state_)\n",
    "        raw_target, smoothed_target, down_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        self.action = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        input_q = [raw,] +  smoothed + down + [self.action,]\n",
    "        self.Q = tf.squeeze(self.critic(input_q))\n",
    "        # target network\n",
    "        # for double q-learning we use actor network not for target network\n",
    "        self.actor_target_output = self.actor([raw_target,] +  smoothed_target + down_target)\n",
    "        input_q_target = [raw_target,] +  smoothed_target + down_target + [self.actor_target_output,]\n",
    "        Q_target = tf.squeeze(self.critic_target(input_q_target))\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name='reward')\n",
    "        target = self.reward  + self.gamma * Q_target\n",
    "        self.target_value = self.reward  + self.gamma * Q_target\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        # get rid of bias of prioritized\n",
    "        self.weights = tf.placeholder(tf.float32, shape=[None], name=\"weights\")\n",
    "        self.loss = tf.reduce_mean(self.weights * tf.square(target - self.Q), name='loss')\n",
    "        # TD-error for priority\n",
    "        self.error = tf.abs(target - self.Q)\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        # build graph for actor training\n",
    "        self.actor_output = self.actor([raw,] +  smoothed + down)\n",
    "        input_q_actor = [raw,] +  smoothed + down + [self.actor_output,]\n",
    "        self.Q_actor = tf.squeeze(self.critic(input_q_actor))\n",
    "        # optimization\n",
    "        self.actor_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(-self.Q_actor, var_list=self.actor.trainable_weights)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        is_initialize = True\n",
    "        if self.is_load:\n",
    "            if self.load(self.save_path):\n",
    "                print('succeded to load')\n",
    "                is_initialize = False\n",
    "            else:\n",
    "                print('failed to load')\n",
    "        \n",
    "        # initialize network\n",
    "        if is_initialize:\n",
    "            tf.global_variables_initializer().run(session=self.sess)\n",
    "            weights = self.critic.get_weights()\n",
    "            self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        \"\"\"Build critic network\n",
    "        \n",
    "        recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        # lower layer\n",
    "        lower_model = [self.build_network(self.model_config['critic_lower'], input_shape=(self.history_length, self.n_stock, 1)) \n",
    "                       for _ in range(1  + self.n_smooth + self.n_down)]\n",
    "        merged = Merge(lower_model, mode='concat')\n",
    "        # upper layer\n",
    "        upper_model = self.build_network(self.model_config['critic_upper'],  model=merged)\n",
    "        # action layer\n",
    "        action = self.build_network(self.model_config['critic_action'], input_shape=(self.n_stock,), is_conv=False)\n",
    "        # output layer\n",
    "        merged = Merge([upper_model, action], mode='mul')\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def build_actor(self):\n",
    "        \"\"\"Build actor network\n",
    "        \n",
    "        recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        # lower layer\n",
    "        lower_model = [self.build_network(self.model_config['actor_lower'], input_shape=(self.history_length, self.n_stock, 1)) \n",
    "                       for _ in range(1  + self.n_smooth + self.n_down)]\n",
    "        merged = Merge(lower_model, mode='concat')\n",
    "        # upper layer\n",
    "        model = self.build_network(self.model_config['actor_upper'],  model=merged)\n",
    "        return model\n",
    "    \n",
    "    def build_network(self, conf, model=None, input_shape=None, is_conv=True):\n",
    "        \"\"\"Build network\"\"\"\n",
    "        _model = model\n",
    "        model = Sequential()\n",
    "        if _model is None:\n",
    "            model.add(Lambda(lambda x: x,  input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(_model)\n",
    "            \n",
    "        for x in conf:\n",
    "            if x['is_drop']:\n",
    "                model.add(Dropout(x['drop_rate']))\n",
    "            if x['type'] is 'full':\n",
    "                if is_conv:\n",
    "                    model.add(Flatten())\n",
    "                    is_conv = False\n",
    "                model.add(Dense(x['n_feature']))\n",
    "            elif x['type'] is 'conv':\n",
    "                model.add(Convolution2D(nb_filter=x['n_feature'], \n",
    "                                        nb_row=x['kw'], \n",
    "                                        nb_col=1, \n",
    "                                        border_mode='same'))  \n",
    "                is_conv=True\n",
    "            if x['is_batch']:\n",
    "                if x['type'] is 'full':\n",
    "                    model.add(BatchNormalization(mode=1, axis=-1))\n",
    "                if x['type'] is 'conv':\n",
    "                    model.add(BatchNormalization(mode=2, axis=-1))\n",
    "            model.add(x['activation'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        \"\"\"Transform data into the Multi Scaled one\n",
    "        \n",
    "        Args:\n",
    "            input: tensor with shape: [None, self.n_history, self.n_stock]\n",
    "        Return:\n",
    "            list of the same shape tensors, [None, self.length_history, self.n_stock]\n",
    "        \"\"\"\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 2):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :]\n",
    "                                        for st in range(n_sm)]),0))\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 2):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] \n",
    "                                for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return raw, smoothed, down\n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        try:\n",
    "            self.saver.restore(self.sess, self.save_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DDPGConfig(object):\n",
    "    def __init__(self, n_stock):\n",
    "        self.device = '/gpu:0'\n",
    "        self.save_path = '/path/to/your/save/path/model.ckpt'\n",
    "        self.is_load = False\n",
    "        self.gamma = 1.0\n",
    "        self.history_length = 10\n",
    "        self.n_stock = n_stock\n",
    "        self.n_smooth = 5\n",
    "        self.n_down = 5\n",
    "        self.n_batch = 32\n",
    "        self.n_epoch = 100\n",
    "        self.update_rate = 1e-1\n",
    "        self.learning_rate = 1e-3\n",
    "        self.model_config = {'critic_lower':[{'type':'conv', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'conv', 'n_feature': 64, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'conv', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False}],\n",
    "                             'critic_upper':[{'type':'full', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'full', 'n_feature': 10, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False}],\n",
    "                             'critic_action':[{'type':'full', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'full', 'n_feature': 10, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False}],\n",
    "                             'actor_lower':[{'type':'conv', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'conv', 'n_feature': 64, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'conv', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False}],\n",
    "                             'actor_upper':[{'type':'full', 'n_feature': 32, 'kw': 4,\n",
    "                                              'activation': PReLU(), 'is_batch': True, 'is_drop': False},\n",
    "                                             {'type':'full', 'n_feature': self.n_stock, 'kw': 4,\n",
    "                                              'activation': Activation('tanh'), 'is_batch': True, 'is_drop': False}]}\n",
    "        self.memory_length = 200\n",
    "        self.n_memory = 10\n",
    "        self.noise_scale = 0.2\n",
    "        self.alpha = 0.7\n",
    "        self.beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model....\n",
      "finished building model!\n",
      "training....\n",
      "time: 2012-06-14 00:00:00\n",
      "portfolio: [ 0.3608433   0.27719364  0.19234474 -0.33030266 -0.44293746  0.95697886\n",
      "  0.19520596  0.16226752 -0.9879598   0.30844563]\n",
      "reward: -2.00139243554\n",
      "value: -2.00139243554\n",
      "elapsed time 28.4270269871\n",
      "********************************************************************\n",
      "time: 2012-06-15 00:00:00\n",
      "portfolio: [ 0.20770501  0.3986474  -0.23503314 -0.51001096 -0.48606864  0.9768818\n",
      "  0.38818526  0.00332157 -0.97650552  0.27818984]\n",
      "reward: -0.322339066465\n",
      "value: -2.323731502\n",
      "elapsed time 55.5785429478\n",
      "********************************************************************\n",
      "time: 2012-06-18 00:00:00\n",
      "portfolio: [-0.23685537  0.31587714 -0.45516613 -0.80889648 -0.67965305  0.98344201\n",
      "  0.84790498 -0.69543564 -0.38401493 -0.33410472]\n",
      "reward: 0.279127565199\n",
      "value: -2.0446039368\n",
      "elapsed time 81.9282770157\n",
      "********************************************************************\n",
      "time: 2012-06-19 00:00:00\n",
      "portfolio: [-0.02292987  0.26991612 -0.31623778 -0.61737788 -0.85019374  0.95260113\n",
      "  0.93645239 -0.82848489 -0.25360516 -0.61421871]\n",
      "reward: 4.28454822757\n",
      "value: 2.23994429076\n",
      "elapsed time 108.004184008\n",
      "********************************************************************\n",
      "time: 2012-06-20 00:00:00\n",
      "portfolio: [-0.50620353  0.44235903 -0.20609452 -0.62720537 -0.64921081  0.9495346\n",
      "  0.9465943  -0.91231519 -0.24378379 -0.46840894]\n",
      "reward: 0.791592385913\n",
      "value: 3.03153667668\n",
      "elapsed time 134.132260799\n",
      "********************************************************************\n",
      "time: 2012-06-21 00:00:00\n",
      "portfolio: [-0.23167507  0.54717708 -0.25553811 -0.7971338  -0.66247833  0.91417658\n",
      "  0.95881939 -0.89383596 -0.20777695 -0.61781526]\n",
      "reward: -1.21701217334\n",
      "value: 1.81452450333\n",
      "elapsed time 160.568072796\n",
      "********************************************************************\n",
      "time: 2012-06-22 00:00:00\n",
      "portfolio: [-0.09199618  0.06115229 -0.62981558 -0.81026864 -0.84613711  0.9118886\n",
      "  0.97321492 -0.60782903 -0.03374434 -0.42274788]\n",
      "reward: -2.95934632041\n",
      "value: -1.14482181708\n",
      "elapsed time 186.845916986\n",
      "********************************************************************\n",
      "time: 2012-06-25 00:00:00\n",
      "portfolio: [-0.17621702 -0.19452235 -0.72834522 -0.84676343 -0.81675029  0.96590066\n",
      "  0.42819998 -0.54173785 -0.46874717  0.06229143]\n",
      "reward: 0.132233833631\n",
      "value: -1.01258798345\n",
      "elapsed time 213.241735935\n",
      "********************************************************************\n",
      "time: 2012-06-26 00:00:00\n",
      "portfolio: [-0.05362165  0.21679093 -0.87781489 -0.43122154 -0.50132859  0.95300418\n",
      " -0.44610849 -0.00169713 -0.90149617 -0.11123721]\n",
      "reward: 1.33079388091\n",
      "value: 0.318205897461\n",
      "elapsed time 240.232083797\n",
      "********************************************************************\n",
      "time: 2012-06-27 00:00:00\n",
      "portfolio: [-0.16923559  0.44144222 -0.97016048  0.15582283 -0.03613574  0.92057192\n",
      " -0.43070248 -0.1036929  -0.91785651  0.15883234]\n",
      "reward: 0.398259008421\n",
      "value: 0.716464905882\n",
      "elapsed time 266.427135944\n",
      "********************************************************************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of /path/to/your/save/path/model.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-885ee1fcef1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_stock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mddpg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-90f31733f5d1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved in file: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tomoaki/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m       raise ValueError(\n\u001b[1;32m-> 1314\u001b[1;33m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of /path/to/your/save/path/model.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "n_stock = input_data.shape[-1]\n",
    "\n",
    "config = DDPGConfig(n_stock)\n",
    "ddpg = DDPG(config)\n",
    "values = ddpg.train(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-48ab06a8724d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tomoaki/work/blog/content\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
